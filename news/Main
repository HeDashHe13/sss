# === INSTALL DEPENDENCIES (Colab only) ===
!pip install -q feedparser gspread oauth2client transformers bs4 requests newspaper3k

# === IMPORTS ===
import feedparser
import requests
from bs4 import BeautifulSoup
from datetime import datetime
import hashlib
import time
import os
import csv
import gspread
from oauth2client.service_account import ServiceAccountCredentials
from transformers import pipeline
from newspaper import Article

# === CONFIGURATION ===
KEYWORDS = ["bitcoin", "ethereum", "solana", "SOL", "sol", "eth", "btc"]
RSS_FEEDS = [
    "https://cointelegraph.com/rss",
    "https://www.coindesk.com/arc/outboundfeeds/rss/"
]
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
}
CSV_FILE = "crypto_structured_dataset.csv"  # Optional local backup
CREDS_FILE = "credentials.json"  # Upload this file
SHEET_ID = "1eInZFtIaLjiCGo-TxFgNZxaHtN4yMmw9NqJkyal6xAE"
SHEET_TAB_NAME = "crypto_structured_dataset"

# === GOOGLE SHEETS SETUP ===
def get_google_sheet():
    scope = ["https://spreadsheets.google.com/feeds", "https://www.googleapis.com/auth/drive"]
    creds = ServiceAccountCredentials.from_json_keyfile_name(CREDS_FILE, scope)
    client = gspread.authorize(creds)
    return client.open_by_key(SHEET_ID).worksheet(SHEET_TAB_NAME)

sheet = get_google_sheet()

# === LOAD SEEN ARTICLES ===
def load_seen_hashes():
    try:
        data = sheet.get_all_values()
        return set(row[0] for row in data[1:] if row)
    except Exception as e:
        print(f"[Error loading hashes] {e}")
        return set()

seen_articles = load_seen_hashes()

# === SUMMARIZER ===
summarizer = pipeline("summarization", model="facebook/bart-large-cnn")

def summarize(text, max_length=130):
    try:
        if len(text) < 200:
            return text
        summary = summarizer(text[:1024], max_length=max_length, min_length=30, do_sample=False)
        return summary[0]['summary_text']
    except Exception as e:
        print(f"[Summary Error] {e}")
        return text[:300] + "..."

# === ARTICLE EXTRACTOR ===
def extract_full_text(url):
    domain = "cointelegraph.com" in url

    if not domain:
        try:
            article = Article(url)
            article.download()
            article.parse()
            text = article.text.strip()
            if len(text) >= 300:
                return text
            print(f"[Too Short (newspaper)] {len(text)} chars → {url}")
        except Exception as e:
            print(f"[Newspaper3k failed] {url} → {e}")

    try:
        response = requests.get(url, headers={
            "User-Agent": (
                "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
                "AppleWebKit/537.36 (KHTML, like Gecko) "
                "Chrome/91.0.4472.124 Safari/537.36"
            ),
            "Accept-Language": "en-US,en;q=0.9"
        }, timeout=10)
        soup = BeautifulSoup(response.text, "html.parser")

        containers = [
            soup.find("div", class_="post-content"),
            soup.find("div", class_="article-content"),
            soup.find("article"),
            soup.find("main"),
            soup.body
        ]

        paragraphs = []
        for container in containers:
            if container:
                paragraphs = container.find_all("p")
                if len(paragraphs) > 2:
                    break

        text = "\n".join(p.get_text(strip=True) for p in paragraphs).strip()
        if len(text) < 300:
            print(f"[Too Short (fallback)] {len(text)} chars → {url}")
            return ""
        return text

    except Exception as e:
        print(f"[Fallback extraction failed] {url} → {e}")
        return ""

# === PROCESSING LOOP ===
def process_feeds():
    found_new = False

    for feed_url in RSS_FEEDS:
        feed = feedparser.parse(feed_url)

        for entry in feed.entries:
            title = entry.title
            link = entry.link
            published = entry.get("published", datetime.utcnow().isoformat())
            source = feed_url

            article_hash = hashlib.md5(link.encode()).hexdigest()
            if article_hash in seen_articles:
                print(f"[Skipped] Duplicate: {title}")
                continue

            print(f"[Processing] {title}")
            full_text = extract_full_text(link)

            if not full_text:
                print("[Warning] Article too short or failed to extract.")
                continue

            keyword_hit = next((k for k in KEYWORDS if k in (title + full_text + link).lower()), None)
            if not keyword_hit:
                print(f"[Skipped] Not related after full text: {title}")
                continue

            summary = summarize(full_text)

            try:
                sheet.append_row([
                    article_hash, title, link, published, source,
                    keyword_hit, summary, full_text, "", ""
                ])
                print(f"[Saved to Sheet] {title}")
                found_new = True
            except Exception as e:
                print(f"[Google Sheets Error] {e}")

            with open(CSV_FILE, "a", newline="", encoding="utf-8") as f:
                writer = csv.writer(f)
                writer.writerow([
                    article_hash, title, link, published, source,
                    keyword_hit, summary, full_text, "", ""
                ])

    if not found_new:
        print("✅ No new matching articles today.")
    else:
        print("✅ Script completed with new entries.")

# === RUN ===
process_feeds()
